# -*- coding: utf-8 -*-
"""util.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1TadC64Bw19bRhv85cJyp0c9blGO6ZHcx
"""

# We'll start with our library imports...
from __future__ import print_function

import os
os.environ['KMP_DUPLICATE_LIB_OK']='True'

import numpy as np                 # to use numpy arrays
import tensorflow as tf            # to specify and run computation graphs
import tensorflow_datasets as tfds # to load training data
import matplotlib.pyplot as plt    # to visualize data and draw plots
from tqdm import tqdm              # to track progress of loops
import re
from bs4 import BeautifulSoup
import unicodedata
from nltk.tokenize.toktok import ToktokTokenizer
from tensorflow.keras.preprocessing import sequence
from tensorflow.keras.preprocessing.text import Tokenizer
import string

def loading_imdb():
    '''
    Required package: TensorFlow
    '''
    (train_ds,test_ds),meta_info = tfds.load('imdb_reviews',with_info=True, split=['train[:60%]','train[80%:]'], batch_size=-1)
    # If batch_size=-1, will return the full dataset as tf.Tensors.
    X_train = train_ds['text']
    y_train = train_ds['label']
    X_test = test_ds['text']
    y_test = test_ds['label']
    
    return X_train, y_train, X_test, y_test

# ### Preprocessing
def remove_html_tags(text):
    soup = BeautifulSoup(text, "html.parser")
    [s.extract() for s in soup(['iframe','script'])]
    stripped_text = soup.get_text()
    stripped_text = re.sub(r'["\|\n|\r|\n\r]+','', stripped_text)
    return stripped_text

#removing accented characters
def remove_accented_chars(text):
    text = unicodedata.normalize('NFKD', text).encode('ascii','ignore').decode('utf-8','ignore')
    return text

#removing special characters:
def remove_special_characters(text):
    text = re.sub('\[.*?\]', '', text)
    text = re.sub('https?://\S+|www\.\S+', '', text)
    text = re.sub('<.*?>+', '', text)
    text = re.sub('[%s]' % re.escape(string.punctuation), '', text)
    text = re.sub('\n', '', text)
    text = re.sub('\w*\d\w*', '', text)
    text = re.sub(r'[^a-zA-z\s]','',text)
    return text

#removing stopwords

def remove_stopwords(text):
    
    tokenizer = ToktokTokenizer()
    stopword_list = ["i", "me", "my", "myself", "we", "our", "ours", "ourselves", "you", 
                 "your", "yours", "yourself", "yourselves", "he", "him", "his", "himself", 
                 "she", "her", "hers", "herself", "it", "its", "itself", "they", "them", 
                 "their", "theirs", "themselves", "what", "which", "who", "whom", "this", 
                 "that", "these", "those", "am", "is", "are", "was", "were", "be", "been", 
                 "being", "have", "has", "had", "having", "do", "does", "did", "doing", "a", 
                 "an", "the", "and", "but", "if", "or", "because", "as", "until", "while", 
                 "of", "at", "by", "for", "with", "about", "against", "between", "into", 
                 "through", "during", "before", "after", "above", "below", "to", "from", 
                 "up", "down", "in", "out", "on", "off", "over", "under", "again", "further", 
                 "then", "once", "here", "there", "when", "where", "why", "how", "all", "any", 
                 "both", "each", "few", "more", "most", "other", "some", "such", "no", "nor", 
                 "not", "only", "own", "same", "so", "than", "too", "very", "s", "t", "can", 
                 "will", "just", "don", "should", "now"]   
    tokens = tokenizer.tokenize(text)
    tokens = [token.strip() for token in tokens]
    filtered_tokens = [token for token in tokens if token not in stopword_list]
    filtered_text = ' '.join(filtered_tokens)    
    
    return filtered_text


def preprocessing_text(text_arr):
    preprocessed_text = []
    idx = 0
    for text in text_arr:
        text = remove_html_tags(text) #remove html tags
        text = remove_accented_chars(text) #removing accented characters
        text = remove_special_characters(text) #removing special characters
        text = text.lower() #change to lower case
        text = remove_stopwords(text) #removing stopwords
        #word_seq,vocab_size = toakenizing(text)
        #max_vocab_size = np.max(vocab_size)
        preprocessed_text.append(text)
        #print("Preprocessing Data, No. %d"%idx)
        idx+=1
    return preprocessed_text
def initial_label(predict):
    predict[predict>=0.5] = 1
    predict[predict<0.5] = 0
    return predict

def show_history(history,key):
	plt.figure()
#	plt.plot(history.history['val_loss'])
	plt.plot(history.history[key])
	plt.title(key)
	plt.ylabel(key)
	plt.xlabel('No. epoch')
	plt.show()
	plt.savefig("./"+key+".jpg")
	np.savetxt(key+".txt", history.history[key])

def plot_diagnostics(record_accuracy, record_val_accuracy):
#plot accuracy curve
    plt.figure()
    line_up = plt.plot(record_accuracy, color='blue',label='train')
    line_down = plt.plot(record_val_accuracy, color='red',
                         label='validation')
    #plt.legend([line_up, line_down],['train', 'validation'])
    plt.legend(bbox_to_anchor=(0., 1.02, 1., .102), loc='lower left',
               ncol=2, mode="expand", borderaxespad=0.)
    plt.xlabel('No. epoch')
    plt.ylabel('Accuracy')
    plt.savefig("./Learning_curve.jpg")

def confidence_interval(error,n):
    low = error -1.96 *np.sqrt((error * (1-error)) / n)
    high = error + 1.96 *np.sqrt((error * (1-error)) / n)
    return low, high

